DeepCloth: Neural Garment Representation for  Shape and Style Editing
=========
[Su et al.](https://www.liuyebin.com/DeepCloth/DeepCloth.html)
> 用统一的框架DeepCloth来用于服装的表现、重建、动画和编辑。
## 1.全文框架
<div align="center">
<img src="/Essay%20Note/images/DeepCloth_overview.png" width=1000 height=250
 />
<br> 图1.1：全文架构
</div>

如图1.1所示：
1. 用一个具有连续mask表示的UV-位置图表征，其中 mask 表示服装的拓扑结构和覆盖区域，而UV图上的渲染纹理表示几何细节。这种表征方法将服装的造型风格和拓扑结构转化为二维UV图，适合于不同服装造型之间的连续过渡。
2. 通过引入ParamNet来执行UV映射编码，其通过使用基于CNN的编码-解码结构将UV映射及其烟吗信息映射到特征空间。
3. 通过改变和差值特征空间中的特征，可以进行服装形状的转换和编辑，并可应用于服装形状推理和动画模块。

具体而言，给定3D服装扫描，服装推理模块可以通过将点云转换为我们的神经服装表征来重建服装形状。将服装映射到服装特征空间后，可以对重构的服装进行服装动画或形状编辑。

## 2. T型服装表征
对于T型服装表征，用一种连续的UV掩码服装表征来将3D服装网格映射到连续的2D UV空间。这样的表征自然地编码了服装在人体上的三维几何分布，而不依赖于固定的模板，即一个服装类型的不同服装造型风格（如长袖/短袖的前开/前闭T恤）可以映射到同一个特征空间中，因此支持不同服装拓扑之间的风格转换和编辑。

在该表征框架中，将服装网络视为覆盖人体表面的几何结构，然后将这些服装映射到标准人体模型UV图上，该图存储了服装拓扑结构和人体的法向距离。通过将3D服装结合映射到2D SMPL UV空间，可以建立服装顶点与人体模型上的最近邻顶点之间的关系，更好地表征几何特征。从给定的SMPL表面发射光线，使服装网格与服装顶点相交，从而建立从服装顶点到SMPL表面点的一一对应映射。

这个SMPL表面点即可被指定为服装顶点$\bar{g}_i^T$对应的子顶点$\bar{v}_i^T$。这样，利用预定义的每个SMPL三角面片的UV坐标，就可以相应找到子顶点$\bar{v}_i^T$的UV坐标来作为服装顶点$\bar{g}_i^T$的UV坐标。

然后将$\bar{v}_i^T$和$\bar{g}_i^T$的距离值设置为渲染纹素，表示相距$\bar{v}_i^T$的法向距离。

通过将每种服装类型（上衣、裤子、连衣裙）表示为一个UV空间，用UV图的掩膜来表示其对人体的覆盖面积，这样就包含了不同形状风格的拓扑信息。下一步是对UV图编码，从而将其应用到形状转换框架中。

## 3. 学习服装形状和风格空间













# end